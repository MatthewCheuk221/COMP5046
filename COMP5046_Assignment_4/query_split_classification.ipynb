{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeNdyflcw-Lg",
        "outputId": "8b48ff43-b5ec-4fe2-dc2f-13ba51fed979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tags(text, variable_map):\n",
        "    tokens = text.split()\n",
        "    tags = []\n",
        "    for token in tokens:\n",
        "        matched = False\n",
        "        for var in variable_map:\n",
        "            if var in token:\n",
        "                tags.append(var)\n",
        "                matched = True\n",
        "                break\n",
        "        if not matched:\n",
        "            tags.append(\"O\")\n",
        "    return tags\n",
        "\n",
        "def preprocess(data):\n",
        "    examples = []\n",
        "    template_map = {}\n",
        "    template_id_counter = 0\n",
        "\n",
        "    for block in data:\n",
        "        sql_templates = block[\"sql\"]\n",
        "        shortest_template = min(sql_templates, key=len).replace('\\n', ' ')\n",
        "\n",
        "        if shortest_template not in template_map:\n",
        "            template_map[shortest_template] = template_id_counter\n",
        "            template_id_counter += 1\n",
        "\n",
        "        for sent in block[\"sentences\"]:\n",
        "            split = block.get(\"query-split\", \"train\")\n",
        "            text = sent[\"text\"]\n",
        "            variables = sent[\"variables\"]\n",
        "            filled_question = text\n",
        "            for var, val in variables.items():\n",
        "                filled_question = filled_question.replace(var, val)\n",
        "\n",
        "            examples.append({\n",
        "                \"split\": split,\n",
        "                \"question\": filled_question,\n",
        "                \"tokens\": text.split(),\n",
        "                \"tags\": get_tags(text, variables),\n",
        "                \"template_sql\": shortest_template,\n",
        "                \"template_id\": template_map[shortest_template],\n",
        "                \"sql_gold\": sql_templates\n",
        "            })\n",
        "\n",
        "    return examples, template_map"
      ],
      "metadata": {
        "id": "CTzmiIZH9WfQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ATISDataset(Dataset):\n",
        "    def __init__(self, data, vocab, tag_vocab):\n",
        "        self.data = data\n",
        "        self.vocab = vocab\n",
        "        self.tag_vocab = tag_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.data[idx]\n",
        "        token_ids = [self.vocab.get(w.lower(), 0) for w in ex[\"tokens\"]]\n",
        "        tag_ids = [self.tag_vocab.get(t, 0) for t in ex[\"tags\"]]\n",
        "        return {\n",
        "            \"tokens\": torch.tensor(token_ids),\n",
        "            \"tags\": torch.tensor(tag_ids),\n",
        "            \"template_id\": ex[\"template_id\"]\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    tokens = nn.utils.rnn.pad_sequence([b[\"tokens\"] for b in batch], batch_first=True)\n",
        "    tags = nn.utils.rnn.pad_sequence([b[\"tags\"] for b in batch], batch_first=True)\n",
        "    template_ids = torch.tensor([b[\"template_id\"] for b in batch])\n",
        "    return {\"tokens\": tokens, \"tags\": tags, \"template\": template_ids}"
      ],
      "metadata": {
        "id": "uE87WWhe9e0N"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tag_size):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, 64)\n",
        "        self.out = nn.Linear(64, tag_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.out(self.emb(x))\n",
        "\n",
        "class LinearClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, num_templates):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, 64)\n",
        "        self.out = nn.Linear(64, num_templates)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.out(self.emb(x).mean(dim=1))"
      ],
      "metadata": {
        "id": "bngactTpCBaU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tag_size):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, 64)\n",
        "        self.mlp = nn.Sequential(nn.Linear(64, 128), nn.ReLU(), nn.Linear(128, tag_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(self.emb(x))\n",
        "\n",
        "class FFClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, num_templates):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, 64)\n",
        "        self.mlp = nn.Sequential(nn.Linear(64, 128), nn.ReLU(), nn.Linear(128, num_templates))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(self.emb(x).mean(dim=1))"
      ],
      "metadata": {
        "id": "D_1x1MnYCCsE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tag_size):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, 128, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(128, 128, batch_first=True, num_layers=1, dropout=0, bidirectional=False)\n",
        "        self.dropout = nn.Dropout(0)\n",
        "        self.tagger = nn.Linear(128, tag_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.emb(x)\n",
        "        out, _ = self.lstm(emb)\n",
        "        return self.tagger(self.dropout(out))\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, num_templates):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, 128, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(128, 128, batch_first=True, num_layers=1, dropout=0, bidirectional=False)\n",
        "        self.dropout = nn.Dropout(0)\n",
        "        self.classifier = nn.Linear(128, num_templates)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.emb(x)\n",
        "        _, (h_n, _) = self.lstm(emb)\n",
        "        return self.classifier(self.dropout(h_n[-1]))"
      ],
      "metadata": {
        "id": "C-qYv7clCL6A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tag_size, max_len=128, nhead=8, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, 128, padding_idx=0)\n",
        "        self.pos_emb = nn.Embedding(max_len, 128)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=nhead, dim_feedforward=256, dropout=0.5, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.tagger = nn.Linear(128, tag_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0).expand(x.size(0), -1)\n",
        "        x = self.emb(x) + self.pos_emb(positions)\n",
        "        return self.tagger(self.transformer(x))\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, num_templates, max_len=128, nhead=8, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, 128, padding_idx=0)\n",
        "        self.pos_emb = nn.Embedding(max_len, 128)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=nhead, dim_feedforward=256, dropout=0.5, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.classifier = nn.Linear(128, num_templates)\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0).expand(x.size(0), -1)\n",
        "        x = self.emb(x) + self.pos_emb(positions)\n",
        "        return self.classifier(self.transformer(x).mean(dim=1))"
      ],
      "metadata": {
        "id": "kbvqovm7COig"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_template_and_tags(name, tagger, classifier, data, vocab, tag_vocab, template_map):\n",
        "    tagger.eval()\n",
        "    classifier.eval()\n",
        "    tag_rev = {v: k for k, v in tag_vocab.items()}\n",
        "\n",
        "    correct_template = 0\n",
        "    correct_tags = 0\n",
        "    total = len(data)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for ex in data:\n",
        "            tokens = ex[\"tokens\"]\n",
        "            token_ids = torch.tensor([vocab.get(w.lower(), 0) for w in tokens]).unsqueeze(0)\n",
        "            pred_template = classifier(token_ids).argmax(dim=1).item()\n",
        "            if pred_template == ex[\"template_id\"]:\n",
        "                correct_template += 1\n",
        "\n",
        "            pred_tags = tagger(token_ids).squeeze(0).argmax(dim=-1).cpu().tolist()\n",
        "            target_tags = [tag_vocab[t] for t in ex[\"tags\"]]\n",
        "            correct_tags += sum([p == g for p, g in zip(pred_tags, target_tags)])\n",
        "\n",
        "    tag_total = sum(len(ex[\"tags\"]) for ex in data)\n",
        "    tag_acc = correct_tags / tag_total\n",
        "    template_acc = correct_template / total\n",
        "\n",
        "    print(f\"\\nTagging Accuracy on \\\"{name.lower()}\\\": {correct_tags}/{tag_total} = {tag_acc:.5f}\")\n",
        "    print(f\"Template Classification Accuracy on \\\"{name.lower()}\\\": {correct_template}/{total} = {template_acc:.5f}\")"
      ],
      "metadata": {
        "id": "v98JL7dNCX3y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"atis.json\") as dataset:\n",
        "    atis_data = json.load(dataset)"
      ],
      "metadata": {
        "id": "9-owII8rCo9R"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_linear(tagger, classifier, dataloader, tag_size, num_templates, epochs=20):\n",
        "    criterion_tag = nn.CrossEntropyLoss()\n",
        "    criterion_template = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(tagger.parameters()) + list(classifier.parameters()), lr=1e-3)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        tagger.train()\n",
        "        classifier.train()\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            tokens, tags, templates = batch[\"tokens\"], batch[\"tags\"], batch[\"template\"]\n",
        "            tag_loss = criterion_tag(tagger(tokens).view(-1, tag_size), tags.view(-1))\n",
        "            template_loss = criterion_template(classifier(tokens), templates)\n",
        "            loss = tag_loss + template_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"[Linear] Epoch {epoch+1}: Loss = {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "examples, template_map = preprocess(atis_data)\n",
        "train_data = [ex for ex in examples if ex[\"split\"] == \"train\"]\n",
        "dev_data = [ex for ex in examples if ex[\"split\"] == \"dev\"]\n",
        "test_data = [ex for ex in examples if ex[\"split\"] == \"test\"]\n",
        "vocab = {\"<PAD>\": 0}\n",
        "tag_vocab = {\"O\": 0}\n",
        "for ex in train_data + dev_data + test_data:\n",
        "    for tok in ex[\"tokens\"]:\n",
        "        vocab.setdefault(tok.lower(), len(vocab))\n",
        "    for tag in ex[\"tags\"]:\n",
        "        tag_vocab.setdefault(tag, len(tag_vocab))\n",
        "dataset = ATISDataset(train_data, vocab, tag_vocab)\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "tagger = LinearTagger(len(vocab), len(tag_vocab))\n",
        "classifier = LinearClassifier(len(vocab), len(template_map))\n",
        "train_linear(tagger, classifier, loader, len(tag_vocab), len(template_map))\n",
        "evaluate_template_and_tags(\"dev\", tagger, classifier, dev_data, vocab, tag_vocab, template_map)\n",
        "evaluate_template_and_tags(\"test\", tagger, classifier, test_data, vocab, tag_vocab, template_map)"
      ],
      "metadata": {
        "id": "XnfnjjJiCmn7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86caa126-362f-45e8-c567-149988add7c8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Linear] Epoch 1: Loss = 6.4668\n",
            "[Linear] Epoch 2: Loss = 4.6847\n",
            "[Linear] Epoch 3: Loss = 4.2751\n",
            "[Linear] Epoch 4: Loss = 3.9803\n",
            "[Linear] Epoch 5: Loss = 3.7053\n",
            "[Linear] Epoch 6: Loss = 3.4566\n",
            "[Linear] Epoch 7: Loss = 3.2271\n",
            "[Linear] Epoch 8: Loss = 3.0082\n",
            "[Linear] Epoch 9: Loss = 2.7976\n",
            "[Linear] Epoch 10: Loss = 2.6385\n",
            "[Linear] Epoch 11: Loss = 2.4338\n",
            "[Linear] Epoch 12: Loss = 2.2908\n",
            "[Linear] Epoch 13: Loss = 2.1414\n",
            "[Linear] Epoch 14: Loss = 2.0041\n",
            "[Linear] Epoch 15: Loss = 1.8747\n",
            "[Linear] Epoch 16: Loss = 1.7531\n",
            "[Linear] Epoch 17: Loss = 1.6453\n",
            "[Linear] Epoch 18: Loss = 1.5510\n",
            "[Linear] Epoch 19: Loss = 1.4452\n",
            "[Linear] Epoch 20: Loss = 1.3505\n",
            "\n",
            "Tagging Accuracy on \"dev\": 1384/1390 = 0.99568\n",
            "Template Classification Accuracy on \"dev\": 0/121 = 0.00000\n",
            "\n",
            "Tagging Accuracy on \"test\": 3933/3947 = 0.99645\n",
            "Template Classification Accuracy on \"test\": 0/347 = 0.00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_feedforward(tagger, classifier, dataloader, tag_size, num_templates, epochs=20):\n",
        "    criterion_tag = nn.CrossEntropyLoss()\n",
        "    criterion_template = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(tagger.parameters()) + list(classifier.parameters()), lr=1e-3)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        tagger.train()\n",
        "        classifier.train()\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            tokens, tags, templates = batch[\"tokens\"], batch[\"tags\"], batch[\"template\"]\n",
        "            tag_loss = criterion_tag(tagger(tokens).view(-1, tag_size), tags.view(-1))\n",
        "            template_loss = criterion_template(classifier(tokens), templates)\n",
        "            loss = tag_loss + template_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"[Feedforward] Epoch {epoch+1}: Loss = {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "examples, template_map = preprocess(atis_data)\n",
        "train_data = [ex for ex in examples if ex[\"split\"] == \"train\"]\n",
        "dev_data = [ex for ex in examples if ex[\"split\"] == \"dev\"]\n",
        "test_data = [ex for ex in examples if ex[\"split\"] == \"test\"]\n",
        "vocab = {\"<PAD>\": 0}\n",
        "tag_vocab = {\"O\": 0}\n",
        "for ex in train_data + dev_data + test_data:\n",
        "    for tok in ex[\"tokens\"]:\n",
        "        vocab.setdefault(tok.lower(), len(vocab))\n",
        "    for tag in ex[\"tags\"]:\n",
        "        tag_vocab.setdefault(tag, len(tag_vocab))\n",
        "dataset = ATISDataset(train_data, vocab, tag_vocab)\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "tagger = FFTagger(len(vocab), len(tag_vocab))\n",
        "classifier = FFClassifier(len(vocab), len(template_map))\n",
        "train_feedforward(tagger, classifier, loader, len(tag_vocab), len(template_map))\n",
        "evaluate_template_and_tags(\"dev\", tagger, classifier, dev_data, vocab, tag_vocab, template_map)\n",
        "evaluate_template_and_tags(\"test\", tagger, classifier, test_data, vocab, tag_vocab, template_map)"
      ],
      "metadata": {
        "id": "LluVlyCxC_dR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86c64b39-5993-4909-9b9e-601bcf79acd6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Feedforward] Epoch 1: Loss = 5.3478\n",
            "[Feedforward] Epoch 2: Loss = 3.9680\n",
            "[Feedforward] Epoch 3: Loss = 3.2969\n",
            "[Feedforward] Epoch 4: Loss = 2.7282\n",
            "[Feedforward] Epoch 5: Loss = 2.2296\n",
            "[Feedforward] Epoch 6: Loss = 1.8039\n",
            "[Feedforward] Epoch 7: Loss = 1.4488\n",
            "[Feedforward] Epoch 8: Loss = 1.1710\n",
            "[Feedforward] Epoch 9: Loss = 0.9376\n",
            "[Feedforward] Epoch 10: Loss = 0.7727\n",
            "[Feedforward] Epoch 11: Loss = 0.6603\n",
            "[Feedforward] Epoch 12: Loss = 0.5550\n",
            "[Feedforward] Epoch 13: Loss = 0.4780\n",
            "[Feedforward] Epoch 14: Loss = 0.4188\n",
            "[Feedforward] Epoch 15: Loss = 0.3655\n",
            "[Feedforward] Epoch 16: Loss = 0.3302\n",
            "[Feedforward] Epoch 17: Loss = 0.2900\n",
            "[Feedforward] Epoch 18: Loss = 0.2624\n",
            "[Feedforward] Epoch 19: Loss = 0.2390\n",
            "[Feedforward] Epoch 20: Loss = 0.2142\n",
            "\n",
            "Tagging Accuracy on \"dev\": 1390/1390 = 1.00000\n",
            "Template Classification Accuracy on \"dev\": 0/121 = 0.00000\n",
            "\n",
            "Tagging Accuracy on \"test\": 3947/3947 = 1.00000\n",
            "Template Classification Accuracy on \"test\": 0/347 = 0.00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lstm(tagger, classifier, dataloader, tag_size, num_templates, epochs=20):\n",
        "    criterion_tag = nn.CrossEntropyLoss()\n",
        "    criterion_template = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(tagger.parameters()) + list(classifier.parameters()), lr=1e-3)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        tagger.train()\n",
        "        classifier.train()\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            tokens, tags, templates = batch[\"tokens\"], batch[\"tags\"], batch[\"template\"]\n",
        "            tag_loss = criterion_tag(tagger(tokens).view(-1, tag_size), tags.view(-1))\n",
        "            template_loss = criterion_template(classifier(tokens), templates)\n",
        "            loss = tag_loss + template_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"[LSTM] Epoch {epoch+1}: Loss = {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "examples, template_map = preprocess(atis_data)\n",
        "train_data = [ex for ex in examples if ex[\"split\"] == \"train\"]\n",
        "dev_data = [ex for ex in examples if ex[\"split\"] == \"dev\"]\n",
        "test_data = [ex for ex in examples if ex[\"split\"] == \"test\"]\n",
        "vocab = {\"<PAD>\": 0}\n",
        "tag_vocab = {\"O\": 0}\n",
        "for ex in train_data + dev_data + test_data:\n",
        "    for tok in ex[\"tokens\"]:\n",
        "        vocab.setdefault(tok.lower(), len(vocab))\n",
        "    for tag in ex[\"tags\"]:\n",
        "        tag_vocab.setdefault(tag, len(tag_vocab))\n",
        "dataset = ATISDataset(train_data, vocab, tag_vocab)\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "tagger = LSTMTagger(len(vocab), len(tag_vocab))\n",
        "classifier = LSTMClassifier(len(vocab), len(template_map))\n",
        "train_lstm(tagger, classifier, loader, len(tag_vocab), len(template_map))\n",
        "evaluate_template_and_tags(\"dev\", tagger, classifier, dev_data, vocab, tag_vocab, template_map)\n",
        "evaluate_template_and_tags(\"test\", tagger, classifier, test_data, vocab, tag_vocab, template_map)"
      ],
      "metadata": {
        "id": "N0MqH5IgDmd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aee0a1c-0e0b-4226-8fc5-0de4359b138d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LSTM] Epoch 1: Loss = 5.5461\n",
            "[LSTM] Epoch 2: Loss = 3.9807\n",
            "[LSTM] Epoch 3: Loss = 3.0319\n",
            "[LSTM] Epoch 4: Loss = 2.3724\n",
            "[LSTM] Epoch 5: Loss = 1.8882\n",
            "[LSTM] Epoch 6: Loss = 1.5598\n",
            "[LSTM] Epoch 7: Loss = 1.2455\n",
            "[LSTM] Epoch 8: Loss = 1.0397\n",
            "[LSTM] Epoch 9: Loss = 0.8302\n",
            "[LSTM] Epoch 10: Loss = 0.6790\n",
            "[LSTM] Epoch 11: Loss = 0.5680\n",
            "[LSTM] Epoch 12: Loss = 0.4665\n",
            "[LSTM] Epoch 13: Loss = 0.3812\n",
            "[LSTM] Epoch 14: Loss = 0.3260\n",
            "[LSTM] Epoch 15: Loss = 0.2667\n",
            "[LSTM] Epoch 16: Loss = 0.2160\n",
            "[LSTM] Epoch 17: Loss = 0.2324\n",
            "[LSTM] Epoch 18: Loss = 0.2104\n",
            "[LSTM] Epoch 19: Loss = 0.1997\n",
            "[LSTM] Epoch 20: Loss = 0.1567\n",
            "\n",
            "Tagging Accuracy on \"dev\": 1390/1390 = 1.00000\n",
            "Template Classification Accuracy on \"dev\": 0/121 = 0.00000\n",
            "\n",
            "Tagging Accuracy on \"test\": 3941/3947 = 0.99848\n",
            "Template Classification Accuracy on \"test\": 0/347 = 0.00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_transformer(tagger, classifier, dataloader, tag_size, num_templates, epochs=20):\n",
        "    criterion_tag = nn.CrossEntropyLoss()\n",
        "    criterion_template = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(tagger.parameters()) + list(classifier.parameters()), lr=1e-3)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        tagger.train()\n",
        "        classifier.train()\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            tokens, tags, templates = batch[\"tokens\"], batch[\"tags\"], batch[\"template\"]\n",
        "            tag_loss = criterion_tag(tagger(tokens).view(-1, tag_size), tags.view(-1))\n",
        "            template_loss = criterion_template(classifier(tokens), templates)\n",
        "            loss = tag_loss + template_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"[Transformer] Epoch {epoch+1}: Loss = {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "examples, template_map = preprocess(atis_data)\n",
        "train_data = [ex for ex in examples if ex[\"split\"] == \"train\"]\n",
        "dev_data = [ex for ex in examples if ex[\"split\"] == \"dev\"]\n",
        "test_data = [ex for ex in examples if ex[\"split\"] == \"test\"]\n",
        "vocab = {\"<PAD>\": 0}\n",
        "tag_vocab = {\"O\": 0}\n",
        "for ex in train_data + dev_data + test_data:\n",
        "    for tok in ex[\"tokens\"]:\n",
        "        vocab.setdefault(tok.lower(), len(vocab))\n",
        "    for tag in ex[\"tags\"]:\n",
        "        tag_vocab.setdefault(tag, len(tag_vocab))\n",
        "dataset = ATISDataset(train_data, vocab, tag_vocab)\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "tagger = TransformerTagger(len(vocab), len(tag_vocab))\n",
        "classifier = TransformerClassifier(len(vocab), len(template_map))\n",
        "train_transformer(tagger, classifier, loader, len(tag_vocab), len(template_map))\n",
        "evaluate_template_and_tags(\"dev\", tagger, classifier, dev_data, vocab, tag_vocab, template_map)\n",
        "evaluate_template_and_tags(\"test\", tagger, classifier, test_data, vocab, tag_vocab, template_map)"
      ],
      "metadata": {
        "id": "ubbDn4ECD3n_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57e2797f-8be6-4dc8-8d9e-951b8843654e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Transformer] Epoch 1: Loss = 4.8375\n",
            "[Transformer] Epoch 2: Loss = 3.0456\n",
            "[Transformer] Epoch 3: Loss = 2.2298\n",
            "[Transformer] Epoch 4: Loss = 1.7113\n",
            "[Transformer] Epoch 5: Loss = 1.3237\n",
            "[Transformer] Epoch 6: Loss = 1.0222\n",
            "[Transformer] Epoch 7: Loss = 0.7708\n",
            "[Transformer] Epoch 8: Loss = 0.5752\n",
            "[Transformer] Epoch 9: Loss = 0.4358\n",
            "[Transformer] Epoch 10: Loss = 0.3149\n",
            "[Transformer] Epoch 11: Loss = 0.2457\n",
            "[Transformer] Epoch 12: Loss = 0.2047\n",
            "[Transformer] Epoch 13: Loss = 0.1767\n",
            "[Transformer] Epoch 14: Loss = 0.1482\n",
            "[Transformer] Epoch 15: Loss = 0.1255\n",
            "[Transformer] Epoch 16: Loss = 0.1136\n",
            "[Transformer] Epoch 17: Loss = 0.1067\n",
            "[Transformer] Epoch 18: Loss = 0.0971\n",
            "[Transformer] Epoch 19: Loss = 0.0905\n",
            "[Transformer] Epoch 20: Loss = 0.0903\n",
            "\n",
            "Tagging Accuracy on \"dev\": 1383/1390 = 0.99496\n",
            "Template Classification Accuracy on \"dev\": 0/121 = 0.00000\n",
            "\n",
            "Tagging Accuracy on \"test\": 3947/3947 = 1.00000\n",
            "Template Classification Accuracy on \"test\": 0/347 = 0.00000\n"
          ]
        }
      ]
    }
  ]
}