{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeNdyflcw-Lg",
        "outputId": "bb3825ee-43c2-4ba9-e387-59c7b8851703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tags(text, variable_map):\n",
        "    tokens = text.split()\n",
        "    tags = []\n",
        "    for token in tokens:\n",
        "        matched = False\n",
        "        for var in variable_map:\n",
        "            if var in token:\n",
        "                tags.append(var)\n",
        "                matched = True\n",
        "                break\n",
        "        if not matched:\n",
        "            tags.append(\"O\")\n",
        "    return tags\n",
        "\n",
        "def preprocess(data):\n",
        "    examples = []\n",
        "    template_map = {}\n",
        "    template_id_counter = 0\n",
        "\n",
        "    for block in data:\n",
        "        sql_templates = block[\"sql\"]\n",
        "        shortest_template = min(sql_templates, key=len).replace('\\n', ' ')\n",
        "\n",
        "        if shortest_template not in template_map:\n",
        "            template_map[shortest_template] = template_id_counter\n",
        "            template_id_counter += 1\n",
        "\n",
        "        for sent in block[\"sentences\"]:\n",
        "            split = sent.get(\"question-split\", \"train\")\n",
        "            text = sent[\"text\"]\n",
        "            variables = sent[\"variables\"]\n",
        "            filled_question = text\n",
        "            for var, val in variables.items():\n",
        "                filled_question = filled_question.replace(var, val)\n",
        "\n",
        "            examples.append({\n",
        "                \"split\": split,\n",
        "                \"question\": filled_question,\n",
        "                \"tokens\": text.split(),\n",
        "                \"tags\": get_tags(text, variables),\n",
        "                \"template_sql\": shortest_template,\n",
        "                \"template_id\": template_map[shortest_template],\n",
        "                \"sql_gold\": sql_templates\n",
        "            })\n",
        "\n",
        "    return examples, template_map"
      ],
      "metadata": {
        "id": "Ax3nG300zTuf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ATISDataset(Dataset):\n",
        "    def __init__(self, data, vocab, tag_vocab):\n",
        "        self.data = data\n",
        "        self.vocab = vocab\n",
        "        self.tag_vocab = tag_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.data[idx]\n",
        "        token_ids = [self.vocab.get(w.lower(), 0) for w in ex[\"tokens\"]]\n",
        "        tag_ids = [self.tag_vocab.get(t, 0) for t in ex[\"tags\"]]\n",
        "        return {\n",
        "            \"tokens\": torch.tensor(token_ids),\n",
        "            \"tags\": torch.tensor(tag_ids),\n",
        "            \"template_id\": ex[\"template_id\"]\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    tokens = nn.utils.rnn.pad_sequence([b[\"tokens\"] for b in batch], batch_first=True)\n",
        "    tags = nn.utils.rnn.pad_sequence([b[\"tags\"] for b in batch], batch_first=True)\n",
        "    template_ids = torch.tensor([b[\"template_id\"] for b in batch])\n",
        "    return {\"tokens\": tokens, \"tags\": tags, \"template\": template_ids}"
      ],
      "metadata": {
        "id": "cXHcKPEnzV4W"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tag_size):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, 64)\n",
        "        self.out = nn.Linear(64, tag_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.out(self.emb(x))\n",
        "\n",
        "class LinearClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, num_templates):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, 64)\n",
        "        self.out = nn.Linear(64, num_templates)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.out(self.emb(x).mean(dim=1))"
      ],
      "metadata": {
        "id": "ZXcoQfzNznrL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tag_size):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, 64)\n",
        "        self.mlp = nn.Sequential(nn.Linear(64, 128), nn.ReLU(), nn.Linear(128, tag_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(self.emb(x))\n",
        "\n",
        "class FFClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, num_templates):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, 64)\n",
        "        self.mlp = nn.Sequential(nn.Linear(64, 128), nn.ReLU(), nn.Linear(128, num_templates))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(self.emb(x).mean(dim=1))"
      ],
      "metadata": {
        "id": "Xmk8WggxzuLL"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tag_size):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, 128, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(128, 128, batch_first=True, num_layers=1, dropout=0, bidirectional=False)\n",
        "        self.dropout = nn.Dropout(0)\n",
        "        self.tagger = nn.Linear(128, tag_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.emb(x)\n",
        "        out, _ = self.lstm(emb)\n",
        "        return self.tagger(self.dropout(out))\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, num_templates):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, 128, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(128, 128, batch_first=True, num_layers=1, dropout=0, bidirectional=False)\n",
        "        self.dropout = nn.Dropout(0)\n",
        "        self.classifier = nn.Linear(128, num_templates)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.emb(x)\n",
        "        _, (h_n, _) = self.lstm(emb)\n",
        "        return self.classifier(self.dropout(h_n[-1]))"
      ],
      "metadata": {
        "id": "bGEjsejyzxEj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tag_size, max_len=128, nhead=8, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, 128, padding_idx=0)\n",
        "        self.pos_emb = nn.Embedding(max_len, 128)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=nhead, dim_feedforward=256, dropout=0.5, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.tagger = nn.Linear(128, tag_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0).expand(x.size(0), -1)\n",
        "        x = self.emb(x) + self.pos_emb(positions)\n",
        "        return self.tagger(self.transformer(x))\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, num_templates, max_len=128, nhead=8, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, 128, padding_idx=0)\n",
        "        self.pos_emb = nn.Embedding(max_len, 128)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=nhead, dim_feedforward=256, dropout=0.5, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.classifier = nn.Linear(128, num_templates)\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0).expand(x.size(0), -1)\n",
        "        x = self.emb(x) + self.pos_emb(positions)\n",
        "        return self.classifier(self.transformer(x).mean(dim=1))"
      ],
      "metadata": {
        "id": "o5szHNnYz2lW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_template_and_tags(name, tagger, classifier, data, vocab, tag_vocab, template_map):\n",
        "    tagger.eval()\n",
        "    classifier.eval()\n",
        "    tag_rev = {v: k for k, v in tag_vocab.items()}\n",
        "\n",
        "    correct_template = 0\n",
        "    correct_tags = 0\n",
        "    total = len(data)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for ex in data:\n",
        "            tokens = ex[\"tokens\"]\n",
        "            token_ids = torch.tensor([vocab.get(w.lower(), 0) for w in tokens]).unsqueeze(0)\n",
        "            pred_template = classifier(token_ids).argmax(dim=1).item()\n",
        "            if pred_template == ex[\"template_id\"]:\n",
        "                correct_template += 1\n",
        "\n",
        "            pred_tags = tagger(token_ids).squeeze(0).argmax(dim=-1).cpu().tolist()\n",
        "            target_tags = [tag_vocab[t] for t in ex[\"tags\"]]\n",
        "            correct_tags += sum([p == g for p, g in zip(pred_tags, target_tags)])\n",
        "\n",
        "    tag_total = sum(len(ex[\"tags\"]) for ex in data)\n",
        "    tag_acc = correct_tags / tag_total\n",
        "    template_acc = correct_template / total\n",
        "\n",
        "    print(f\"\\nTagging Accuracy on \\\"{name.lower()}\\\": {correct_tags}/{tag_total} = {tag_acc:.5f}\")\n",
        "    print(f\"Template Classification Accuracy on \\\"{name.lower()}\\\": {correct_template}/{total} = {template_acc:.5f}\")"
      ],
      "metadata": {
        "id": "ISFJVoa1z7JR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"atis.json\") as dataset:\n",
        "    atis_data = json.load(dataset)"
      ],
      "metadata": {
        "id": "JQOImKIPIXxg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_linear(tagger, classifier, dataloader, tag_size, num_templates, epochs=20):\n",
        "    criterion_tag = nn.CrossEntropyLoss()\n",
        "    criterion_template = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(tagger.parameters()) + list(classifier.parameters()), lr=1e-3)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        tagger.train()\n",
        "        classifier.train()\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            tokens, tags, templates = batch[\"tokens\"], batch[\"tags\"], batch[\"template\"]\n",
        "            tag_loss = criterion_tag(tagger(tokens).view(-1, tag_size), tags.view(-1))\n",
        "            template_loss = criterion_template(classifier(tokens), templates)\n",
        "            loss = tag_loss + template_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"[Linear] Epoch {epoch+1}: Loss = {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "examples, template_map = preprocess(atis_data)\n",
        "train_data = [ex for ex in examples if ex[\"split\"] == \"train\"]\n",
        "dev_data = [ex for ex in examples if ex[\"split\"] == \"dev\"]\n",
        "test_data = [ex for ex in examples if ex[\"split\"] == \"test\"]\n",
        "vocab = {\"<PAD>\": 0}\n",
        "tag_vocab = {\"O\": 0}\n",
        "for ex in train_data + dev_data + test_data:\n",
        "    for tok in ex[\"tokens\"]:\n",
        "        vocab.setdefault(tok.lower(), len(vocab))\n",
        "    for tag in ex[\"tags\"]:\n",
        "        tag_vocab.setdefault(tag, len(tag_vocab))\n",
        "dataset = ATISDataset(train_data, vocab, tag_vocab)\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "tagger = LinearTagger(len(vocab), len(tag_vocab))\n",
        "classifier = LinearClassifier(len(vocab), len(template_map))\n",
        "train_linear(tagger, classifier, loader, len(tag_vocab), len(template_map))\n",
        "evaluate_template_and_tags(\"dev\", tagger, classifier, dev_data, vocab, tag_vocab, template_map)\n",
        "evaluate_template_and_tags(\"test\", tagger, classifier, test_data, vocab, tag_vocab, template_map)"
      ],
      "metadata": {
        "id": "yzUtv7AV0q17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab6f073-947a-4c49-ef76-bc8ad4d4b72a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Linear] Epoch 1: Loss = 6.7569\n",
            "[Linear] Epoch 2: Loss = 4.9411\n",
            "[Linear] Epoch 3: Loss = 4.5473\n",
            "[Linear] Epoch 4: Loss = 4.2800\n",
            "[Linear] Epoch 5: Loss = 4.0296\n",
            "[Linear] Epoch 6: Loss = 3.8115\n",
            "[Linear] Epoch 7: Loss = 3.5926\n",
            "[Linear] Epoch 8: Loss = 3.3861\n",
            "[Linear] Epoch 9: Loss = 3.1779\n",
            "[Linear] Epoch 10: Loss = 3.0107\n",
            "[Linear] Epoch 11: Loss = 2.8301\n",
            "[Linear] Epoch 12: Loss = 2.6644\n",
            "[Linear] Epoch 13: Loss = 2.5029\n",
            "[Linear] Epoch 14: Loss = 2.3646\n",
            "[Linear] Epoch 15: Loss = 2.2378\n",
            "[Linear] Epoch 16: Loss = 2.1042\n",
            "[Linear] Epoch 17: Loss = 1.9750\n",
            "[Linear] Epoch 18: Loss = 1.8582\n",
            "[Linear] Epoch 19: Loss = 1.7506\n",
            "[Linear] Epoch 20: Loss = 1.6474\n",
            "\n",
            "Tagging Accuracy on \"dev\": 5169/5179 = 0.99807\n",
            "Template Classification Accuracy on \"dev\": 306/486 = 0.62963\n",
            "\n",
            "Tagging Accuracy on \"test\": 4002/4015 = 0.99676\n",
            "Template Classification Accuracy on \"test\": 192/447 = 0.42953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_feedforward(tagger, classifier, dataloader, tag_size, num_templates, epochs=20):\n",
        "    criterion_tag = nn.CrossEntropyLoss()\n",
        "    criterion_template = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(tagger.parameters()) + list(classifier.parameters()), lr=1e-3)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        tagger.train()\n",
        "        classifier.train()\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            tokens, tags, templates = batch[\"tokens\"], batch[\"tags\"], batch[\"template\"]\n",
        "            tag_loss = criterion_tag(tagger(tokens).view(-1, tag_size), tags.view(-1))\n",
        "            template_loss = criterion_template(classifier(tokens), templates)\n",
        "            loss = tag_loss + template_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"[Feedforward] Epoch {epoch+1}: Loss = {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "examples, template_map = preprocess(atis_data)\n",
        "train_data = [ex for ex in examples if ex[\"split\"] == \"train\"]\n",
        "dev_data = [ex for ex in examples if ex[\"split\"] == \"dev\"]\n",
        "test_data = [ex for ex in examples if ex[\"split\"] == \"test\"]\n",
        "vocab = {\"<PAD>\": 0}\n",
        "tag_vocab = {\"O\": 0}\n",
        "for ex in train_data + dev_data + test_data:\n",
        "    for tok in ex[\"tokens\"]:\n",
        "        vocab.setdefault(tok.lower(), len(vocab))\n",
        "    for tag in ex[\"tags\"]:\n",
        "        tag_vocab.setdefault(tag, len(tag_vocab))\n",
        "dataset = ATISDataset(train_data, vocab, tag_vocab)\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "tagger = FFTagger(len(vocab), len(tag_vocab))\n",
        "classifier = FFClassifier(len(vocab), len(template_map))\n",
        "train_feedforward(tagger, classifier, loader, len(tag_vocab), len(template_map))\n",
        "evaluate_template_and_tags(\"dev\", tagger, classifier, dev_data, vocab, tag_vocab, template_map)\n",
        "evaluate_template_and_tags(\"test\", tagger, classifier, test_data, vocab, tag_vocab, template_map)"
      ],
      "metadata": {
        "id": "Lld-xtZu06k-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b019457c-b500-4e8a-d88a-e96baf60013c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Feedforward] Epoch 1: Loss = 5.5219\n",
            "[Feedforward] Epoch 2: Loss = 4.2376\n",
            "[Feedforward] Epoch 3: Loss = 3.6420\n",
            "[Feedforward] Epoch 4: Loss = 3.1008\n",
            "[Feedforward] Epoch 5: Loss = 2.6266\n",
            "[Feedforward] Epoch 6: Loss = 2.1927\n",
            "[Feedforward] Epoch 7: Loss = 1.8119\n",
            "[Feedforward] Epoch 8: Loss = 1.4852\n",
            "[Feedforward] Epoch 9: Loss = 1.2242\n",
            "[Feedforward] Epoch 10: Loss = 1.0077\n",
            "[Feedforward] Epoch 11: Loss = 0.8347\n",
            "[Feedforward] Epoch 12: Loss = 0.7018\n",
            "[Feedforward] Epoch 13: Loss = 0.5903\n",
            "[Feedforward] Epoch 14: Loss = 0.5215\n",
            "[Feedforward] Epoch 15: Loss = 0.4440\n",
            "[Feedforward] Epoch 16: Loss = 0.4066\n",
            "[Feedforward] Epoch 17: Loss = 0.3546\n",
            "[Feedforward] Epoch 18: Loss = 0.3249\n",
            "[Feedforward] Epoch 19: Loss = 0.2886\n",
            "[Feedforward] Epoch 20: Loss = 0.2610\n",
            "\n",
            "Tagging Accuracy on \"dev\": 5179/5179 = 1.00000\n",
            "Template Classification Accuracy on \"dev\": 337/486 = 0.69342\n",
            "\n",
            "Tagging Accuracy on \"test\": 4013/4015 = 0.99950\n",
            "Template Classification Accuracy on \"test\": 215/447 = 0.48098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lstm(tagger, classifier, dataloader, tag_size, num_templates, epochs=20):\n",
        "    criterion_tag = nn.CrossEntropyLoss()\n",
        "    criterion_template = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(tagger.parameters()) + list(classifier.parameters()), lr=1e-3)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        tagger.train()\n",
        "        classifier.train()\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            tokens, tags, templates = batch[\"tokens\"], batch[\"tags\"], batch[\"template\"]\n",
        "            tag_loss = criterion_tag(tagger(tokens).view(-1, tag_size), tags.view(-1))\n",
        "            template_loss = criterion_template(classifier(tokens), templates)\n",
        "            loss = tag_loss + template_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"[LSTM] Epoch {epoch+1}: Loss = {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "examples, template_map = preprocess(atis_data)\n",
        "train_data = [ex for ex in examples if ex[\"split\"] == \"train\"]\n",
        "dev_data = [ex for ex in examples if ex[\"split\"] == \"dev\"]\n",
        "test_data = [ex for ex in examples if ex[\"split\"] == \"test\"]\n",
        "vocab = {\"<PAD>\": 0}\n",
        "tag_vocab = {\"O\": 0}\n",
        "for ex in train_data + dev_data + test_data:\n",
        "    for tok in ex[\"tokens\"]:\n",
        "        vocab.setdefault(tok.lower(), len(vocab))\n",
        "    for tag in ex[\"tags\"]:\n",
        "        tag_vocab.setdefault(tag, len(tag_vocab))\n",
        "dataset = ATISDataset(train_data, vocab, tag_vocab)\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "tagger = LSTMTagger(len(vocab), len(tag_vocab))\n",
        "classifier = LSTMClassifier(len(vocab), len(template_map))\n",
        "train_lstm(tagger, classifier, loader, len(tag_vocab), len(template_map))\n",
        "evaluate_template_and_tags(\"dev\", tagger, classifier, dev_data, vocab, tag_vocab, template_map)\n",
        "evaluate_template_and_tags(\"test\", tagger, classifier, test_data, vocab, tag_vocab, template_map)"
      ],
      "metadata": {
        "id": "bIf0uidJ1lXc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d8e67c5-1fda-40ee-ce75-7542b72865bf"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LSTM] Epoch 1: Loss = 5.7310\n",
            "[LSTM] Epoch 2: Loss = 4.2079\n",
            "[LSTM] Epoch 3: Loss = 3.3437\n",
            "[LSTM] Epoch 4: Loss = 2.7265\n",
            "[LSTM] Epoch 5: Loss = 2.2249\n",
            "[LSTM] Epoch 6: Loss = 1.8099\n",
            "[LSTM] Epoch 7: Loss = 1.4537\n",
            "[LSTM] Epoch 8: Loss = 1.2132\n",
            "[LSTM] Epoch 9: Loss = 0.9721\n",
            "[LSTM] Epoch 10: Loss = 0.7859\n",
            "[LSTM] Epoch 11: Loss = 0.6363\n",
            "[LSTM] Epoch 12: Loss = 0.5125\n",
            "[LSTM] Epoch 13: Loss = 0.4000\n",
            "[LSTM] Epoch 14: Loss = 0.3313\n",
            "[LSTM] Epoch 15: Loss = 0.2926\n",
            "[LSTM] Epoch 16: Loss = 0.2932\n",
            "[LSTM] Epoch 17: Loss = 0.2747\n",
            "[LSTM] Epoch 18: Loss = 0.2054\n",
            "[LSTM] Epoch 19: Loss = 0.1567\n",
            "[LSTM] Epoch 20: Loss = 0.1355\n",
            "\n",
            "Tagging Accuracy on \"dev\": 5177/5179 = 0.99961\n",
            "Template Classification Accuracy on \"dev\": 325/486 = 0.66872\n",
            "\n",
            "Tagging Accuracy on \"test\": 4013/4015 = 0.99950\n",
            "Template Classification Accuracy on \"test\": 206/447 = 0.46085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_transformer(tagger, classifier, dataloader, tag_size, num_templates, epochs=20):\n",
        "    criterion_tag = nn.CrossEntropyLoss()\n",
        "    criterion_template = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(tagger.parameters()) + list(classifier.parameters()), lr=1e-3)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        tagger.train()\n",
        "        classifier.train()\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            tokens, tags, templates = batch[\"tokens\"], batch[\"tags\"], batch[\"template\"]\n",
        "            tag_loss = criterion_tag(tagger(tokens).view(-1, tag_size), tags.view(-1))\n",
        "            template_loss = criterion_template(classifier(tokens), templates)\n",
        "            loss = tag_loss + template_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"[Transformer] Epoch {epoch+1}: Loss = {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "examples, template_map = preprocess(atis_data)\n",
        "train_data = [ex for ex in examples if ex[\"split\"] == \"train\"]\n",
        "dev_data = [ex for ex in examples if ex[\"split\"] == \"dev\"]\n",
        "test_data = [ex for ex in examples if ex[\"split\"] == \"test\"]\n",
        "vocab = {\"<PAD>\": 0}\n",
        "tag_vocab = {\"O\": 0}\n",
        "for ex in train_data + dev_data + test_data:\n",
        "    for tok in ex[\"tokens\"]:\n",
        "        vocab.setdefault(tok.lower(), len(vocab))\n",
        "    for tag in ex[\"tags\"]:\n",
        "        tag_vocab.setdefault(tag, len(tag_vocab))\n",
        "dataset = ATISDataset(train_data, vocab, tag_vocab)\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "tagger = TransformerTagger(len(vocab), len(tag_vocab))\n",
        "classifier = TransformerClassifier(len(vocab), len(template_map))\n",
        "train_transformer(tagger, classifier, loader, len(tag_vocab), len(template_map))\n",
        "evaluate_template_and_tags(\"dev\", tagger, classifier, dev_data, vocab, tag_vocab, template_map)\n",
        "evaluate_template_and_tags(\"test\", tagger, classifier, test_data, vocab, tag_vocab, template_map)"
      ],
      "metadata": {
        "id": "e3ZkaB5k1xXj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8420c264-3fd5-4234-f7fb-f77a7fb115e0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Transformer] Epoch 1: Loss = 5.0490\n",
            "[Transformer] Epoch 2: Loss = 3.1613\n",
            "[Transformer] Epoch 3: Loss = 2.3737\n",
            "[Transformer] Epoch 4: Loss = 1.8978\n",
            "[Transformer] Epoch 5: Loss = 1.5472\n",
            "[Transformer] Epoch 6: Loss = 1.2640\n",
            "[Transformer] Epoch 7: Loss = 1.0397\n",
            "[Transformer] Epoch 8: Loss = 0.8323\n",
            "[Transformer] Epoch 9: Loss = 0.6844\n",
            "[Transformer] Epoch 10: Loss = 0.5675\n",
            "[Transformer] Epoch 11: Loss = 0.4571\n",
            "[Transformer] Epoch 12: Loss = 0.3690\n",
            "[Transformer] Epoch 13: Loss = 0.3135\n",
            "[Transformer] Epoch 14: Loss = 0.2738\n",
            "[Transformer] Epoch 15: Loss = 0.2628\n",
            "[Transformer] Epoch 16: Loss = 0.2399\n",
            "[Transformer] Epoch 17: Loss = 0.2106\n",
            "[Transformer] Epoch 18: Loss = 0.1754\n",
            "[Transformer] Epoch 19: Loss = 0.1703\n",
            "[Transformer] Epoch 20: Loss = 0.1660\n",
            "\n",
            "Tagging Accuracy on \"dev\": 5179/5179 = 1.00000\n",
            "Template Classification Accuracy on \"dev\": 361/486 = 0.74280\n",
            "\n",
            "Tagging Accuracy on \"test\": 4009/4015 = 0.99851\n",
            "Template Classification Accuracy on \"test\": 232/447 = 0.51902\n"
          ]
        }
      ]
    }
  ]
}